#!/usr/bin/env python3
"""
AI åˆ†æç®¡é“ V2 æµ‹è¯•æ–‡ä»¶

æµ‹è¯• AnalysisPipelineV2 çš„å®Œæ•´åŠŸèƒ½ï¼š
- ç«¯åˆ°ç«¯åˆ†ææµç¨‹
- æ¨¡å—ç»„åˆ
- Token è¿½è¸ª
- æ€§èƒ½æµ‹è¯•

è¿è¡Œæ–¹å¼:
pytest tests/test_pipeline_v2.py -v -s
"""
import asyncio
import os
import sys
import pytest
from dotenv import load_dotenv

load_dotenv()
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))


# ============ Test Fixtures ============

@pytest.fixture
def sample_posts():
    """æä¾›æµ‹è¯•ç”¨å¸–å­æ ·æœ¬"""
    return [
        {
            "content": "Love this product! Best purchase ever! Amazing quality and fast shipping!",
            "author": "user1",
            "platform": "reddit",
            "url": "https://reddit.com/post1"
        },
        {
            "content": "Great quality, though a bit expensive. Worth it though.",
            "author": "user2",
            "platform": "reddit",
            "url": "https://reddit.com/post2"
        },
        {
            "content": "Not great, not terrible. It's okay for the price.",
            "author": "user3",
            "platform": "reddit",
            "url": "https://reddit.com/post3"
        },
        {
            "content": "Terrible experience! Worst purchase ever. Do NOT buy!",
            "author": "user4",
            "platform": "reddit",
            "url": "https://reddit.com/post4"
        },
        {
            "content": "The battery life is excellent. Lasts all day!",
            "author": "user5",
            "platform": "reddit",
            "url": "https://reddit.com/post5"
        },
    ]


@pytest.fixture
def large_posts():
    """æä¾›å¤§æ‰¹é‡æµ‹è¯•å¸–å­"""
    posts = []
    for i in range(50):
        sentiment = ["good", "bad", "okay"][i % 3]
        posts.append({
            "content": f"This product is {sentiment}. Feature {i%5} is nice. Post number {i+1}.",
            "author": f"user{i+1}",
            "platform": "reddit",
            "url": f"https://reddit.com/post{i+1}"
        })
    return posts


# ============ Unit Tests ============

@pytest.mark.asyncio
async def test_pipeline_initialization():
    """æµ‹è¯•ç®¡é“åˆå§‹åŒ–"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    # æµ‹è¯•é»˜è®¤åˆå§‹åŒ–
    pipeline = AnalysisPipelineV2()
    assert pipeline.sentiment_analyzer is not None
    assert pipeline.opinion_clusterer is not None
    assert pipeline.summarizer is not None

    # æµ‹è¯•æŒ‡å®š provider
    pipeline_openai = AnalysisPipelineV2(provider="openai")
    assert pipeline_openai.sentiment_analyzer is not None

    # æµ‹è¯•å¯ç”¨ Map-Reduce
    pipeline_map_reduce = AnalysisPipelineV2(use_map_reduce=True)
    assert pipeline_map_reduce.use_map_reduce == True

    print("âœ“ AnalysisPipelineV2 åˆå§‹åŒ–æˆåŠŸ")


@pytest.mark.asyncio
async def test_pipeline_reset_tracking():
    """æµ‹è¯•é‡ç½® Token è¿½è¸ª"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    pipeline = AnalysisPipelineV2()

    # é‡ç½®è¿½è¸ª
    pipeline.reset_tracking()

    stats = pipeline.logger.total_input_tokens
    assert stats == 0

    print("âœ“ Token è¿½è¸ªé‡ç½®åŠŸèƒ½æ­£å¸¸")


# ============ Integration Tests ============

@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_full_analysis(sample_posts):
    """æµ‹è¯•å®Œæ•´åˆ†ææµç¨‹"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    pipeline = AnalysisPipelineV2()

    print(f"\nåˆ†æ {len(sample_posts)} æ¡å¸–å­...")

    result = await pipeline.analyze_posts(sample_posts)

    print(f"\nåˆ†æç»“æœ:")
    print(f"  æ•´ä½“æƒ…æ„Ÿ: {result['overall_sentiment']:.1f}/100")
    print(f"  æƒ…æ„Ÿç»“æœæ•°: {len(result['sentiment_results'])}")
    print(f"  èšç±»æ•°: {len(result['clusters'])}")
    print(f"  æ‘˜è¦é•¿åº¦: {len(result['summary'])} å­—ç¬¦")
    print(f"\nToken ä½¿ç”¨:")
    print(f"  æ€»è®¡: {result['token_usage']['total']}")
    print(f"  æˆæœ¬: ${result['token_usage']['cost']:.4f}")

    # éªŒè¯ç»“æœç»“æ„
    assert "sentiment_results" in result
    assert "overall_sentiment" in result
    assert "clusters" in result
    assert "summary" in result
    assert "token_usage" in result

    # éªŒè¯æƒ…æ„Ÿåˆ†æ
    assert len(result["sentiment_results"]) == len(sample_posts)
    assert 0 <= result["overall_sentiment"] <= 100

    # éªŒè¯èšç±»
    assert isinstance(result["clusters"], list)

    # éªŒè¯æ‘˜è¦
    assert isinstance(result["summary"], str)
    assert len(result["summary"]) > 0

    # éªŒè¯ Token ä½¿ç”¨
    assert result["token_usage"]["total"] > 0
    assert result["token_usage"]["cost"] > 0

    print("\nâœ“ å®Œæ•´åˆ†ææµç¨‹æ­£å¸¸")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_empty_posts():
    """æµ‹è¯•ç©ºå¸–å­åˆ—è¡¨"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    pipeline = AnalysisPipelineV2()
    result = await pipeline.analyze_posts([])

    print(f"\nç©ºå¸–å­åˆ†æç»“æœ:")
    print(f"  æ•´ä½“æƒ…æ„Ÿ: {result['overall_sentiment']}")
    print(f"  æƒ…æ„Ÿç»“æœ: {len(result['sentiment_results'])}")
    print(f"  èšç±»: {len(result['clusters'])}")

    # éªŒè¯é»˜è®¤å€¼
    assert result["overall_sentiment"] == 50.0
    assert len(result["sentiment_results"]) == 0
    assert result["summary"] == "No posts to analyze."

    print("âœ“ ç©ºå¸–å­å¤„ç†æ­£ç¡®")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_sentiment_only(sample_posts):
    """æµ‹è¯•ä»…æƒ…æ„Ÿåˆ†ææ¨¡å¼"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    pipeline = AnalysisPipelineV2()

    result = await pipeline.analyze_sentiment_only(sample_posts)

    print(f"\nä»…æƒ…æ„Ÿåˆ†æç»“æœ:")
    print(f"  æ•´ä½“æƒ…æ„Ÿ: {result['overall_sentiment']:.1f}/100")
    print(f"  æƒ…æ„Ÿç»“æœæ•°: {len(result['sentiment_results'])}")

    # éªŒè¯ç»“æœ
    assert "sentiment_results" in result
    assert "overall_sentiment" in result
    assert len(result["sentiment_results"]) == len(sample_posts)

    # éªŒè¯æ²¡æœ‰èšç±»å’Œæ‘˜è¦
    assert "clusters" not in result
    assert "summary" not in result

    print("âœ“ ä»…æƒ…æ„Ÿåˆ†ææ¨¡å¼æ­£å¸¸")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_with_options(sample_posts):
    """æµ‹è¯•å¸¦é€‰é¡¹çš„åˆ†æ"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    pipeline = AnalysisPipelineV2()

    # è·³è¿‡èšç±»
    result = await pipeline.analyze_posts(sample_posts, options={
        "skip_clustering": True
    })

    print(f"\nè·³è¿‡èšç±»:")
    print(f"  èšç±»æ•°: {len(result['clusters'])}")
    assert len(result["clusters"]) == 0

    # è·³è¿‡æ‘˜è¦
    result = await pipeline.analyze_posts(sample_posts, options={
        "skip_summary": True
    })

    print(f"\nè·³è¿‡æ‘˜è¦:")
    print(f"  æ‘˜è¦: {result['summary']}")
    assert result["summary"] is None

    # ä¿®æ”¹èšç±»æ•°é‡
    result = await pipeline.analyze_posts(sample_posts, options={
        "top_n_clusters": 1
    })

    print(f"\nTop 1 èšç±»:")
    print(f"  èšç±»æ•°: {len(result['clusters'])}")
    assert len(result["clusters"]) <= 1

    print("âœ“ é€‰é¡¹åŠŸèƒ½æ­£å¸¸")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_with_map_reduce():
    """æµ‹è¯• Map-Reduce æ¨¡å¼"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    pipeline = AnalysisPipelineV2(use_map_reduce=True)

    # åˆ›å»ºå¤§æ•°æ®é›†
    large_dataset = [
        {"content": f"Post {i}: " + "This is content. " * 50}
        for i in range(30)
    ]

    print(f"\nä½¿ç”¨ Map-Reduce å¤„ç† {len(large_dataset)} æ¡å¸–å­...")

    result = await pipeline.analyze_posts(large_dataset)

    print(f"\nåˆ†æç»“æœ:")
    print(f"  æ•´ä½“æƒ…æ„Ÿ: {result['overall_sentiment']:.1f}/100")
    print(f"  Token ä½¿ç”¨: {result['token_usage']['total']}")

    # éªŒè¯ç»“æœ
    assert isinstance(result, dict)
    assert "sentiment_results" in result

    print("âœ“ Map-Reduce æ¨¡å¼æ­£å¸¸")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_large_dataset(large_posts):
    """æµ‹è¯•å¤§æ•°æ®é›†å¤„ç†"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    pipeline = AnalysisPipelineV2()

    print(f"\nå¤„ç† {len(large_posts)} æ¡å¸–å­...")

    result = await pipeline.analyze_posts(large_posts)

    print(f"\nåˆ†æç»“æœ:")
    print(f"  æ•´ä½“æƒ…æ„Ÿ: {result['overall_sentiment']:.1f}/100")
    print(f"  æƒ…æ„Ÿç»“æœæ•°: {len(result['sentiment_results'])}")
    print(f"  èšç±»æ•°: {len(result['clusters'])}")
    print(f"  æ‘˜è¦é•¿åº¦: {len(result['summary'])} å­—ç¬¦")
    print(f"  Token æ€»è®¡: {result['token_usage']['total']}")
    print(f"  API è°ƒç”¨: {result['token_usage']['api_calls']}")

    # éªŒè¯ç»“æœ
    assert len(result["sentiment_results"]) == len(large_posts)
    assert result["token_usage"]["api_calls"] > 0

    print("âœ“ å¤§æ•°æ®é›†å¤„ç†æ­£å¸¸")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_different_providers(sample_posts):
    """æµ‹è¯•ä¸åŒæä¾›å•†"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    providers = ["openai", "tongyi"]

    for provider in providers:
        print(f"\næµ‹è¯• provider: {provider}")

        try:
            pipeline = AnalysisPipelineV2(provider=provider)
            result = await pipeline.analyze_sentiment_only(sample_posts)

            print(f"  æ•´ä½“æƒ…æ„Ÿ: {result['overall_sentiment']:.1f}/100")
            print(f"  âœ“ {provider} å·¥ä½œæ­£å¸¸")

        except Exception as e:
            print(f"  âœ— {provider} å¤±è´¥: {e}")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_result_structure(sample_posts):
    """æµ‹è¯•ç»“æœç»“æ„çš„å®Œæ•´æ€§"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    pipeline = AnalysisPipelineV2()
    result = await pipeline.analyze_posts(sample_posts)

    print(f"\néªŒè¯ç»“æœç»“æ„:")

    # éªŒè¯æƒ…æ„Ÿç»“æœ
    assert isinstance(result["sentiment_results"], list)
    for i, sentiment in enumerate(result["sentiment_results"]):
        assert "score" in sentiment, f"æƒ…æ„Ÿç»“æœ {i} ç¼ºå°‘ score"
        assert "label" in sentiment, f"æƒ…æ„Ÿç»“æœ {i} ç¼ºå°‘ label"
        assert "confidence" in sentiment, f"æƒ…æ„Ÿç»“æœ {i} ç¼ºå°‘ confidence"
        print(f"  æƒ…æ„Ÿ {i}: âœ“")

    # éªŒè¯èšç±»
    assert isinstance(result["clusters"], list)
    for i, cluster in enumerate(result["clusters"]):
        assert "label" in cluster, f"èšç±» {i} ç¼ºå°‘ label"
        assert "summary" in cluster, f"èšç±» {i} ç¼ºå°‘ summary"
        assert "mention_count" in cluster, f"èšç±» {i} ç¼ºå°‘ mention_count"
        print(f"  èšç±» {i}: âœ“")

    # éªŒè¯ Token ä½¿ç”¨
    token_usage = result["token_usage"]
    assert "total" in token_usage
    assert "input" in token_usage
    assert "output" in token_usage
    assert "cost" in token_usage
    assert "api_calls" in token_usage
    print(f"  Token ä½¿ç”¨: âœ“")

    print("âœ“ ç»“æœç»“æ„å®Œæ•´")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_sentiment_distribution(sample_posts):
    """æµ‹è¯•æƒ…æ„Ÿåˆ†å¸ƒ"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    pipeline = AnalysisPipelineV2()
    result = await pipeline.analyze_posts(sample_posts)

    # ç»Ÿè®¡æƒ…æ„Ÿåˆ†å¸ƒ
    labels = [r["label"] for r in result["sentiment_results"]]
    positive_count = labels.count("positive")
    negative_count = labels.count("negative")
    neutral_count = labels.count("neutral")

    print(f"\næƒ…æ„Ÿåˆ†å¸ƒ:")
    print(f"  ç§¯æ: {positive_count}")
    print(f"  æ¶ˆæ: {negative_count}")
    print(f"  ä¸­æ€§: {neutral_count}")
    print(f"  æ€»è®¡: {len(labels)}")

    # éªŒè¯æ€»æ•°
    assert positive_count + negative_count + neutral_count == len(labels)

    print("âœ“ æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡æ­£å¸¸")


# ============ Performance Tests ============

@pytest.mark.performance
@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_performance():
    """æµ‹è¯•ç®¡é“æ€§èƒ½"""
    import time
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    pipeline = AnalysisPipelineV2()

    # æµ‹è¯•å°æ•°æ®é›†
    small_posts = [{"content": f"Test post {i}"} for i in range(5)]

    start = time.time()
    result_small = await pipeline.analyze_posts(small_posts)
    small_duration = time.time() - start

    print(f"\nå°æ•°æ®é›† (5æ¡):")
    print(f"  è€—æ—¶: {small_duration:.2f}s")
    print(f"  Token: {result_small['token_usage']['total']}")
    print(f"  å¹³å‡æ¯æ¡: {small_duration/5:.2f}s")

    # æµ‹è¯•ä¸­ç­‰æ•°æ®é›†
    medium_posts = [{"content": f"Test post {i} with more content. " * 10} for i in range(20)]

    start = time.time()
    result_medium = await pipeline.analyze_posts(medium_posts)
    medium_duration = time.time() - start

    print(f"\nä¸­ç­‰æ•°æ®é›† (20æ¡):")
    print(f"  è€—æ—¶: {medium_duration:.2f}s")
    print(f"  Token: {result_medium['token_usage']['total']}")
    print(f"  å¹³å‡æ¯æ¡: {medium_duration/20:.2f}s")

    print("\nâœ“ æ€§èƒ½æµ‹è¯•å®Œæˆ")


# ============ End-to-End Tests ============

@pytest.mark.integration
@pytest.mark.asyncio
async def test_pipeline_end_to_end():
    """ç«¯åˆ°ç«¯æµ‹è¯•ï¼šæ¨¡æ‹ŸçœŸå®ä½¿ç”¨åœºæ™¯"""
    from src.ai_analysis.pipeline_v2 import AnalysisPipelineV2

    print("\n" + "="*60)
    print("ç«¯åˆ°ç«¯æµ‹è¯•ï¼šæ¨¡æ‹ŸçœŸå®ä½¿ç”¨åœºæ™¯")
    print("="*60)

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼ˆReddit äº§å“è®¨è®ºï¼‰
    reddit_posts = [
        {
            "content": "Just got my new MacBook Pro M3 Max. Absolutely blown away by the performance! The battery life is incredible - getting 15+ hours easily.",
            "author": "techfan123",
            "platform": "reddit",
            "url": "https://reddit.com/r/apple/post1"
        },
        {
            "content": "The M3 Max is overpriced for what you get. My M1 Pro still runs circles around most tasks. Not worth upgrading unless you do heavy video work.",
            "author": "budget_user",
            "platform": "reddit",
            "url": "https://reddit.com/r/apple/post2"
        },
        {
            "content": "The screen is gorgeous but I'm disappointed by the RAM pricing. 8GB should not be the base in 2024. Otherwise solid machine.",
            "author": "designer_pro",
            "platform": "reddit",
            "url": "https://reddit.com/r/apple/post3"
        },
        {
            "content": "Coming from a PC, this is my first Mac. The build quality is premium but macOS is taking some getting used to. Overall happy with the purchase!",
            "author": "switcher_2024",
            "platform": "reddit",
            "url": "https://reddit.com/r/apple/post4"
        },
        {
            "content": "Had heating issues under heavy load. Apple Care replaced it - new unit runs much cooler. Great customer service!",
            "author": "video_editor",
            "platform": "reddit",
            "url": "https://reddit.com/r/apple/post5"
        },
    ]

    print(f"\nåˆ†ææ¥æº: Reddit r/apple")
    print(f"å¸–å­æ•°é‡: {len(reddit_posts)}")
    print(f"ä¸»é¢˜: MacBook Pro M3 Max\n")

    # æ‰§è¡Œåˆ†æ
    pipeline = AnalysisPipelineV2(provider="openai")
    result = await pipeline.analyze_posts(reddit_posts)

    # æ˜¾ç¤ºç»“æœ
    print("\n" + "="*60)
    print("åˆ†æç»“æœ")
    print("="*60)

    print(f"\nğŸ“Š æ•´ä½“æƒ…æ„Ÿ: {result['overall_sentiment']:.1f}/100")

    print(f"\nğŸ¯ ä¸»è¦è§‚ç‚¹èšç±» ({len(result['clusters'])}ä¸ª):")
    for i, cluster in enumerate(result['clusters'], 1):
        print(f"\n  {i}. {cluster['label']}")
        print(f"     {cluster['summary'][:80]}...")
        print(f"     æåŠæ•°: {cluster['mention_count']}")

    print(f"\nğŸ“ è®¨è®ºæ‘˜è¦:")
    print(f"  {result['summary']}")

    print(f"\nğŸ’° Token ä½¿ç”¨:")
    print(f"  æ€» Tokens: {result['token_usage']['total']:,}")
    print(f"  è¾“å…¥: {result['token_usage']['input']:,}")
    print(f"  è¾“å‡º: {result['token_usage']['output']:,}")
    print(f"  API è°ƒç”¨: {result['token_usage']['api_calls']}")
    print(f"  é¢„ä¼°æˆæœ¬: ${result['token_usage']['cost']:.4f}")

    # éªŒè¯ç»“æœè´¨é‡
    assert result['overall_sentiment'] > 0
    assert len(result['clusters']) > 0
    assert len(result['summary']) > 50

    print("\n" + "="*60)
    print("âœ… ç«¯åˆ°ç«¯æµ‹è¯•æˆåŠŸ")
    print("="*60 + "\n")


# ============ Main Function ============

async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*60)
    print("AI åˆ†æç®¡é“ V2 å®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("="*60 + "\n")

    # è¿è¡Œå•å…ƒæµ‹è¯•
    tests = [
        ("åˆå§‹åŒ–æµ‹è¯•", test_pipeline_initialization),
        ("Token è¿½è¸ªé‡ç½®", test_pipeline_reset_tracking),
    ]

    for name, test_func in tests:
        try:
            await test_func()
        except Exception as e:
            print(f"âœ— {name} å¤±è´¥: {e}")

    print("\næç¤º: è¿è¡Œé›†æˆæµ‹è¯•è¯·ä½¿ç”¨:")
    print("  pytest tests/test_pipeline_v2.py -m integration -v -s")
    print("\nè¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•:")
    print("  pytest tests/test_pipeline_v2.py::test_pipeline_end_to_end -v -s")


if __name__ == "__main__":
    asyncio.run(run_all_tests())
