"""
è¯Šæ–­ Reddit å¸–å­è·³è¿‡é—®é¢˜

æŸ¥çœ‹ä¸ºä»€ä¹ˆå¾ˆå¤šå¸–å­è¢«è·³è¿‡
"""
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import quote


def diagnose_skipping():
    """è¯Šæ–­ä¸ºä»€ä¹ˆå¸–å­è¢«è·³è¿‡"""

    keyword = "artificial intelligence"
    search_url = f"https://www.reddit.com/search/?q={quote(keyword)}&type=posts"

    print("=" * 60)
    print("Reddit å¸–å­è·³è¿‡è¯Šæ–­")
    print("=" * 60)

    # é…ç½® Chrome
    chrome_options = Options()
    chrome_options.add_argument("--user-data-dir=chrome_profile")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_argument(f"--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.set_window_size(1920, 1080)

    try:
        driver.get(search_url)
        print("\nç­‰å¾…é¡µé¢åŠ è½½...")
        time.sleep(5)

        print("\n" + "=" * 60)
        print("åˆ†æå¸–å­å…ƒç´ ")
        print("=" * 60)

        # æŸ¥æ‰¾å¸–å­
        posts1 = driver.find_elements("css selector", '[data-testid="search-post-unit"]')
        posts2 = driver.find_elements("css selector", '[data-testid="search-post-with-content-preview"]')

        print(f"\né€‰æ‹©å™¨1: [data-testid=\"search-post-unit\"] - {len(posts1)} ä¸ª")
        print(f"é€‰æ‹©å™¨2: [data-testid=\"search-post-with-content-preview\"] - {len(posts2)} ä¸ª")

        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤
        all_posts = posts1 + posts2
        print(f"æ€»è®¡: {len(all_posts)} ä¸ªå…ƒç´ ")

        # åˆ†ææ¯ä¸ªå¸–å­
        print(f"\n" + "=" * 60)
        print("è¯¦ç»†åˆ†ææ¯ä¸ªå¸–å­å…ƒç´ ")
        print("=" * 60)

        valid_posts = 0
        no_url = 0
        no_title = 0
        duplicate_urls = []

        seen_urls = {}

        for i, post in enumerate(all_posts[:20], 1):  # åªçœ‹å‰20ä¸ª
            print(f"\n--- å¸–å­ {i} ---")

            # å°è¯•è·å– URL
            try:
                link = post.find_element("css selector", '[data-testid="post-title"]')
                url = link.get_attribute('href')
                if url:
                    print(f"âœ… URL: {url}")
                    if url in seen_urls:
                        duplicate_urls.append(url)
                        print(f"âš ï¸  é‡å¤çš„ URL (ä¹‹å‰æ˜¯å¸–å­ {seen_urls[url]})")
                    else:
                        seen_urls[url] = i
                else:
                    print(f"âŒ URL å…ƒç´ å­˜åœ¨ä½† href ä¸ºç©º")
                    no_url += 1
            except Exception as e:
                print(f"âŒ æ— æ³•è·å– URL: {e}")
                no_url += 1

            # å°è¯•è·å–æ ‡é¢˜
            try:
                title_elem = post.find_element("css selector", '[data-testid="post-title-text"]')
                title = title_elem.text
                print(f"âœ… æ ‡é¢˜: {title[:50]}")
                valid_posts += 1
            except Exception as e:
                print(f"âŒ æ— æ³•è·å–æ ‡é¢˜: {e}")
                no_title += 1

            # å°è¯•è·å–å­ç‰ˆ
            try:
                sub_link = post.find_element("css selector", 'a[href^="/r/"]')
                subreddit = sub_link.text
                print(f"âœ… å­ç‰ˆ: {subreddit}")
            except Exception as e:
                print(f"âŒ æ— æ³•è·å–å­ç‰ˆ: {e}")

        print(f"\n" + "=" * 60)
        print("ç»Ÿè®¡ç»“æœ")
        print("=" * 60)
        print(f"æœ‰æ•ˆå¸–å­: {valid_posts}")
        print(f"æ²¡æœ‰ URL: {no_url}")
        print(f"æ²¡æœ‰æ ‡é¢˜: {no_title}")
        print(f"é‡å¤ URL æ•°é‡: {len(duplicate_urls)}")

        if duplicate_urls:
            print(f"\né‡å¤çš„ URLs:")
            for url in set(duplicate_urls):
                print(f"  - {url}")

        print(f"\nğŸ’¡ é—®é¢˜åˆ†æ:")
        if no_url > 0:
            print(f"  âš ï¸  {no_url} ä¸ªå¸–å­æ²¡æœ‰ URLï¼Œä¼šè¢«è·³è¿‡")
        if len(duplicate_urls) > 0:
            print(f"  âš ï¸  ä¸¤ä¸ªé€‰æ‹©å™¨è¿”å›äº†ç›¸åŒçš„å…ƒç´ ï¼")
            print(f"      è¿™æ˜¯ä¸»è¦é—®é¢˜ - ä½¿ç”¨ä¸¤ä¸ªé€‰æ‹©å™¨ä¼šå¯¼è‡´é‡å¤")

        print(f"\nğŸ”§ å»ºè®®çš„ä¿®å¤:")
        print(f"  1. åªä½¿ç”¨ä¸€ä¸ªé€‰æ‹©å™¨")
        print(f"  2. æˆ–è€…åˆå¹¶ä¸¤ä¸ªé€‰æ‹©å™¨çš„ç»“æœå¹¶å»é‡")

        input("\næŒ‰ Enter é”®å…³é—­æµè§ˆå™¨...")

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

    finally:
        driver.quit()


if __name__ == "__main__":
    diagnose_skipping()
